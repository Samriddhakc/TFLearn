{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier: cats vs. dogs\n",
    "# dataset: https://www.kaggle.com/c/dogs-vs-cats\n",
    "\n",
    "# ---------------------\n",
    "# import required packages\n",
    "# ---------------------\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# define constants\n",
    "# ---------------------\n",
    "'''from keras.datasets import mnist\n",
    "trainset= (X_train, y_train)=  mnist.load_data()\n",
    "testset =(X_test, y_test) =  mnist.load_data()'''\n",
    "\n",
    "trainset = ('/home/rabina7/Downloads/MNIST/trainset.csv')\n",
    "testset = ('/home/rabina7/Downloads/MNIST/testset.csv')\n",
    "with open(trainset, 'r') as csvfile: \n",
    "    # creating a csv reader object \n",
    "    csvreader = csv.reader(csvfile) \n",
    "with open(testset, 'r') as csvfile: \n",
    "    # creating a csv reader object \n",
    "    csvreader = csv.reader(csvfile) \n",
    "    \n",
    "#filters out every digits except 0 and 1\n",
    "train_filter = np.where((trainset == 0) | (trainset == 1))\n",
    "test_filter = np.where((testset == 0) | (testset == 1))\n",
    "    \n",
    "IMG_SIZE = 28 # resize all images to the same size\n",
    "LR = 1e-3 # learning rate: 0.001\n",
    "\n",
    "# model name, for model loading and saving\n",
    "MODEL_NAME = 'MNIST-{}-{}.model'.format(LR, '6conv-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of our model is an array consists 2 elements: [catniss, dogniss]\n",
    "# each element represents the probability of being each animal.\n",
    "# e.g., [1, 0] means this image is 100% a cat image and 0% a dog image.\n",
    "# So, the final label of the image is cat.\n",
    "\n",
    "categories = ['0', '1']\n",
    "\n",
    "def img_label(img):\n",
    "    # get the true label of the image\n",
    "    # images in training set are named in form of <animal>.xx.jpg, xx are numbers.\n",
    "    print(\"img path: \"+img)\n",
    "    parts = img.split('.')\n",
    "    print(\"length of parts: \"+str(len(parts)))\n",
    "    print(\"parts: \"+str(parts))\n",
    "    \n",
    "    text_label = img.split('.')[-1]\n",
    "    \n",
    "    if text_label == categories[0]:\n",
    "        return [0, 1]\n",
    "    elif text_label == categories[1]:\n",
    "        return [1, 0]\n",
    "    else:\n",
    "        raise ValueError('The image does not belong to any category! ' + text_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# process training data\n",
    "# ---------------------\n",
    "def process_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.path.join(trainset)):\n",
    "        # full path of the image\n",
    "        img_full_path = os.path.join(trainset, img)\n",
    "        # true label of the image\n",
    "        true_label = img_label(img)\n",
    "        # load the image\n",
    "        img = cv2.imread(img_full_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # resize the image\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        # add new training data, in form of\n",
    "        # (img, label)\n",
    "        training_data.append([np.array(img), np.array(true_label)])\n",
    "        #training_data.append([np.array(img), img_idx])\n",
    "    # shuffle the dataset\n",
    "    shuffle(training_data)\n",
    "    # save the dataset so that next time we can load it directly\n",
    "    np.save('trainset-{}.npy'.format(IMG_SIZE), training_data)\n",
    "    \n",
    "    return training_data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# process testing data\n",
    "# ---------------------\n",
    "def process_test_data():\n",
    "    testing_data = []\n",
    "    for img in tqdm(os.listdir(testset)):\n",
    "        # full path of the image\n",
    "        img_full_path = os.path.join(testset, img)\n",
    "        #img_idx = img.split('.')[0]\n",
    "        true_label = img_label(img)\n",
    "         # load the image\n",
    "        img = cv2.imread(img_full_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # resize the image\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        # add new testing data, in form of\n",
    "        # (img, img_idx)\n",
    "        testing_data.append([np.array(img), np.array(true_label)])\n",
    "        #calculating the testing\n",
    "        #simple loop that goes through all the \n",
    "        #API to predict the label and compare to the true label\n",
    "        #counter\n",
    "    shuffle(testing_data)\n",
    "    # save the dataset so that next time we can load it directly\n",
    "    np.save('testset-{}.npy'.format(IMG_SIZE), testing_data)\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# load training data\n",
    "# ---------------------\n",
    "def load_train_data(load_exist):\n",
    "    trainset = None\n",
    "    if load_exist and os.path.exists('trainset-{}.npy'.format(IMG_SIZE)):\n",
    "        # if want to load an existing training set\n",
    "        trainset = np.load('trainset-{}.npy'.format(IMG_SIZE), allow_pickle = True)\n",
    "    else:\n",
    "        trainset = process_train_data()\n",
    "    return trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rabina7/anaconda3/envs/opencv/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# construct the cnn model for this project\n",
    "# ---------------------\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "def conv_net():\n",
    "    convnet = input_data(shape = [None, IMG_SIZE, IMG_SIZE, 1],\n",
    "                        name = 'input')\n",
    "    # conv_2d(incoming, nb_filter, filter_size, ..., activation)\n",
    "    convnet = conv_2d(convnet, 32, 5, activation = 'relu')\n",
    "    # max_pool_2d(incoming, kernel_size, ...)\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "    \n",
    "    convnet = conv_2d(convnet, 64, 5, activation = 'relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "   \n",
    "    #flatten the layer\n",
    "    flatten = tflearn.flatten(convnet)\n",
    "    # fully_connected(incoming, n_units, activation, ...)\n",
    "    convnet = fully_connected(convnet, 1024, activation = 'relu')\n",
    "    # dropout(incoming, keep_prob is drop_prob + keep+prob)\n",
    "    convnet = dropout(convnet, 0.8)\n",
    "    \n",
    "    #standard recommendation for the Net arch    \n",
    "    logits = fully_connected(convnet, 2, activation = 'softmax')\n",
    "    #convnet = dropout(convnet, 0.8)\n",
    "    \n",
    "    #logits\n",
    "    #logits = tf.layers.dense(inputs=convnet, units=10)\n",
    "    \n",
    "    #regression(incoming, optimizer, learning_rate, loss, name, ...)\n",
    "    convnet = regression(logits, optimizer = 'adam', learning_rate = LR,\n",
    "                             loss = 'categorical_crossentropy', name = 'targets')\n",
    "\n",
    "    return convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From build/bdist.linux-x86_64/egg/tflearn/initializations.py:119: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From build/bdist.linux-x86_64/egg/tflearn/layers/core.py:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From build/bdist.linux-x86_64/egg/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/rabina7/anaconda3/envs/opencv/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# define the model\n",
    "# ---------------------\n",
    "convnet = conv_net()\n",
    "model = tflearn.DNN(convnet, tensorboard_dir = 'log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img path: /\n",
      "length of parts: 1\n",
      "parts: ['/']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The image does not belong to any category! /",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d3ba9e11f7e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# and splitting the testing data and validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdivide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-086a01957eae>\u001b[0m in \u001b[0;36mload_train_data\u001b[0;34m(load_exist)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainset-{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-17b3ce345a98>\u001b[0m in \u001b[0;36mprocess_train_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimg_full_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# true label of the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_full_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b4b76abc6070>\u001b[0m in \u001b[0;36mimg_label\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The image does not belong to any category! '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The image does not belong to any category! /"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# train the model\n",
    "# ---------------------\n",
    "\n",
    "# load model if exists\n",
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print ('loaded existing model', MODEL_NAME)\n",
    "else:\n",
    "    # Loading the training data-set after pre-processing\n",
    "    # and splitting the testing data and validation data\n",
    "    \n",
    "    train_data = load_train_data(False)\n",
    "    print(np.array(train_data).shape)\n",
    "    divide = (len(train_data)/5)\n",
    "    for i in range(5):\n",
    "        start_id = (divide * i)\n",
    "        print(\"startid: \" + str(start_id))\n",
    "        end_id = (divide * (i + 1) -1)\n",
    "        print(\"endid: \" + str(end_id))\n",
    "        validation = train_data[start_id: end_id]\n",
    "        if (i == 0):\n",
    "            train = train_data[end_id +1 : (len(train_data)-1)]\n",
    "            #print(\"Train0:\" + str(train))\n",
    "        if (i == 1 and i == 2 and i == 3):\n",
    "            train = train_data[0: (start_id)-1].merge[end_id +1 : (len(train_data)-1)]\n",
    "        if(i == 4):\n",
    "            train = train = train_data[0 : start_id -1]\n",
    "        # Setting up the features and lables\n",
    "        # x-Features & y-Labels\n",
    "        train_x = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "        train_y = [i[1] for i in train]\n",
    "        validation_x = np.array([i[0] for i in validation]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "        validation_y = [i[1] for i in validation]\n",
    "        # Training the CNN model with the data-sets\n",
    "        model.fit({'input': train_x}, {'targets': train_y}, n_epoch = 3,\n",
    "                            validation_set = ({'input': validation_x}, {'targets': validation_y}),\n",
    "                            snapshot_step = 500, show_metric = True, run_id = MODEL_NAME)\n",
    "\n",
    "    # save the trained model\n",
    "    #model.save(MODEL_NAME)\n",
    "    #enough training/testing sample of each class\n",
    "    #JSMA\n",
    "    #debug on a different MNISTdataset/ standard arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''refined_x=train_x.flatten().reshape(7505, 2500)\n",
    "train_y_labels=[]\n",
    "for i in range(len(train_y)):\n",
    "    train_y_labels.append(train_y[i][0])\n",
    "    train_y_labels=np.array(train_y_labels)\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(refined_x,train_y_labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''prediction=logisticRegr.predict(refined_x)\n",
    "count=0\n",
    "total_train = len(prediction)\n",
    "for i in range(len(prediction)):\n",
    "    if (prediction[i]==train_y_labels[i]):\n",
    "        print(\"prediction{}:, Train{}: \".format(prediction, train_y_labels))\n",
    "        count=count+1 \n",
    "accuracy = (1.0 * count / total_train)             \n",
    "print(\"The training accuracy is {}.\".format(accuracy))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model acheives 90% accuracy, save it if not saved yet\n",
    "if not os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# load testing data\n",
    "# ---------------------\n",
    "def load_test_data(load_exist):\n",
    "    testset = None\n",
    "    if load_exist and os.path.exists('testset-{}.npy'.format(IMG_SIZE)):\n",
    "        # if want to load an existing testing set\n",
    "        testset = np.load('testset-{}.npy'.format(IMG_SIZE), allow_pickle = True)\n",
    "    else: \n",
    "        testset = process_test_data()\n",
    "    return testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_test_data(False)\n",
    "test_x = np.array([i[0] for i in test_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "test_y = [i[1] for i in test_data] \n",
    "\n",
    "predictions = model.predict(test_x)\n",
    "total_test_samples = 0 #len(test_data)\n",
    "\n",
    "counter = 0\n",
    "for prediction, true_label in zip(predictions, test_y):\n",
    "    total_test_samples +=1\n",
    "    predicted_class = np.argmax(prediction) # [0.6, 0.4] #predicts whichever is max\n",
    "    actual_class = np.argmax(true_label)\n",
    "    print(\"predicted:\" + str(prediction))\n",
    "    print(\"true_lable:\"+str(true_label))# [0, 1], [1, 0]\n",
    "    if(predicted_class == actual_class):\n",
    "        counter+=1\n",
    "        \n",
    "    print(\"predicate class: {}, True label: {}, counter {} \".format(predicted_class, actual_class, counter))\n",
    "\n",
    "# For integers, ‘/’ operator returns the quotient. For example, 3/10 returns 0.\n",
    "\n",
    "# So, add 1.0 before counter to convert it to a float number. \n",
    "\n",
    "accuracy = 1.0 * counter / total_test_samples\n",
    "print(total_test_samples)\n",
    "\n",
    "print(\"The testing accuracy is {}.\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# use trained model to predict given image\n",
    "# -------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_data = load_test_data(True)\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "\n",
    "# test and plot 12 images\n",
    "for i, data in enumerate(test_data[:12]):\n",
    "    # cat: [1, 0]; dog: [0, 1]\n",
    "    img_data = data[0]\n",
    "    img_idx = data[1]\n",
    "    \n",
    "    sub_fig = fig.add_subplot(3, 4, i + 1)\n",
    "    \n",
    "    orig_data = img_data\n",
    "    # flatten image\n",
    "    data = img_data.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "    \n",
    "    output = model.predict([data])[0]\n",
    "    \n",
    "    if np.argmax(output) == 1:\n",
    "        pred_label = 'dog'\n",
    "    else:\n",
    "        pred_label = 'cat'\n",
    "    \n",
    "    sub_fig.imshow(orig_data, cmap = 'gray')\n",
    "    \n",
    "    # predicted label and confidence\n",
    "    plt.title(pred_label + '; ' + str(np.max(output)))\n",
    "    sub_fig.get_xaxis().set_visible(False)\n",
    "    sub_fig.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
